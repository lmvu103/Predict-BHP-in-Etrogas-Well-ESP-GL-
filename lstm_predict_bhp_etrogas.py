# -*- coding: utf-8 -*-
"""LSTM_Predict_BHP_Etrogas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YpjC9WZ0yBsB3qAvlwEYPiCpgecokNlc
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.dates as md
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
from scipy import stats
from mpl_toolkits.axes_grid1 import host_subplot
import mpl_toolkits.axisartist as AA
from sklearn.metrics import mean_squared_error
from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.covariance import EllipticEnvelope
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from mpl_toolkits.mplot3d import Axes3D
from numpy import sqrt, random, array, argsort
from keras.models import Sequential
from keras.layers import Dense,LSTM
# %matplotlib inline

##Matplotlib Parameters
import matplotlib.ticker as ticker
from pylab import rcParams
rcParams['axes.labelsize'] = 12
rcParams['xtick.labelsize'] = 12
rcParams['ytick.labelsize'] = 12
plt.style.context('fivethirtyeight')

df = pd.read_csv("/content/DD6P.csv")

df.head(5)

#df.tail(5)

#df.info

#df['Date'].describe()

#df.dtypes

#df.describe

##Deleting  Row with Error Values
#data = df[['Oil rate','THP','BHP','ESP']]
df = df[df['BHP']>0]
df[df.columns[5]] = df[df.columns[5]].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int).dropna()

##Checking Missing Values
df.isnull().sum()

##Filling Missing Values with Previous Day Value
df = df.fillna(method='ffill')
df.isnull().sum()

##Converting Date column as DateTime format
df['Date'] = pd.to_datetime(df['Date'],format="%m/%d/%Y")
df

##Setting Date as Index
df.set_index('Date',inplace=True)
df

# plot the data
df.plot( y='BHP', figsize=(12,6))
plt.xlabel('Date time')
plt.ylabel('BHP (psi)')
plt.title('Time Series of BHP by date time of search');

sns.pairplot(df)

##Line Plots Target & Features
data = df
col_names = data.columns

fig = plt.figure(figsize=(24, 24))
plt.style.use('fivethirtyeight')
for i in range(6):
  ax = fig.add_subplot(6,1,i+1)
  ax.plot(data.iloc[:,i],label=col_names[i])
  data.iloc[:,i].rolling(100).mean().plot(label='Rolling Mean')
  ax.set_title(col_names[i])
  ax.xaxis.set_major_locator(md.YearLocator(base=1))
  ax.xaxis.set_major_formatter(md.DateFormatter("%Y"))
  ax.xaxis.set_tick_params(reset=True)
  ax.set_xlabel('Date')
  ax.set_ylabel('Price')
  plt.legend()
fig.tight_layout(pad=3.0)
plt.show()

#Clustering-Based Anomaly Detection
# k-means algorithm
n_cluster = range(1, 20)
kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]
scores = [kmeans[i].score(data) for i in range(len(kmeans))]

fig, ax = plt.subplots(figsize=(10,6))
ax.plot(n_cluster, scores)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show();

X = df[['THP', 'BHP', 'Oil rate','ESP']]
X = X[X['BHP']>0]
X = X.reset_index(drop=True)
km = KMeans(n_clusters=10)
km.fit(X)
km.predict(X)
labels = km.labels_
#Plotting
fig = plt.figure(1, figsize=(7,7))
ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)
ax.scatter(X.iloc[:,0], X.iloc[:,1], X.iloc[:,2],
          c=labels.astype(np.float), edgecolor="k")
ax.set_xlabel("THP")
ax.set_ylabel("BHP")
ax.set_zlabel("Oil rate")
plt.title("K Means", fontsize=14);

from sklearn.preprocessing import StandardScaler
X = data.values
X_std = StandardScaler().fit_transform(X)
mean_vec = np.mean(X_std, axis=0)
cov_mat = np.cov(X_std.T)
eig_vals, eig_vecs = np.linalg.eig(cov_mat)
eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]
eig_pairs.sort(key = lambda x: x[0], reverse= True)
tot = sum(eig_vals)
var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance

plt.figure(figsize=(10, 5))
plt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='individual explained variance', color = 'g')
plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.legend(loc='best')
plt.show();

# Take useful feature and standardize them
data = df[['BHP', 'THP', 'Oil rate']]
X_std = StandardScaler().fit_transform(X)
data = pd.DataFrame(X_std)
# reduce to 2 important features
pca = PCA(n_components=2)
data = pca.fit_transform(data)
# standardize these 2 new features
scaler = StandardScaler()
np_scaled = scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)

kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]
df['cluster'] = kmeans[9].predict(data)
df.index = data.index
df['principal_feature1'] = data[0]
df['principal_feature2'] = data[1]
df['cluster'].value_counts()

df.head()

"""Clustering Approach"""

# 1. Remove Outlier follow upper and lower bounds
lower_limit = df.BHP.mean() - 2*df.BHP.std()
lower_limit
upper_limit = df.BHP.mean() + 2*df.BHP.std()
upper_limit
df_no_outlier_std_dev = df[(df.BHP<upper_limit) & (df.BHP>lower_limit)]
plt.plot(df.Date,df.BHP,c='g')
plt.plot(df_no_outlier_std_dev.Date,df_no_outlier_std_dev.BHP,c='r')
df_no_outlier_std_dev.shape

# 2. Remove Outlier z-score
df['zscore'] = ( df.BHP- df.BHP.mean() ) / df.BHP.std()
df_no_outliers = df[(df.zscore>-2) & (df.zscore<2)]
plt.plot(df.Date,df.BHP,c='g')
plt.plot(df_no_outliers.Date,df_no_outliers.BHP)
df_no_outliers.shape

#3. Isolation Forest for anomaly detection.
data = df[['BHP', 'THP', 'Oil rate']]
scaler = StandardScaler()
np_scaled = scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)
# train isolation forest
model =  IsolationForest(contamination=outliers_fraction)
model.fit(data)

df['anomaly2'] = pd.Series(model.predict(data))
# df['anomaly2'] = df['anomaly2'].map( {1: 0, -1: 1} )

fig, ax = plt.subplots(figsize=(10,6))

a = df.loc[df['anomaly2'] == -1, ['Date', 'BHP']] #anomaly

ax.plot(df['Date'], df['BHP'], color='blue', label = 'Normal', linestyle='dashed',linewidth=2)
ax.scatter(a['Date'],a['BHP'], color='red', label = 'Anomaly',s=20, marker='o')
plt.legend()
plt.show();

# 4.Support Vector Machine-Based Anomaly DetectionÂ¶
scaler = StandardScaler()
np_scaled = scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)
# train oneclassSVM 
model = OneClassSVM(nu=outliers_fraction, kernel="rbf", gamma=0.05)
model.fit(data)
 
df['anomaly3'] = pd.Series(model.predict(data))
df
# df['anomaly3'] = df['anomaly3'].map( {1: 0, -1: 1} )
fig, ax = plt.subplots(figsize=(10,6))

a = df.loc[df['anomaly3'] == -1, ['Date', 'BHP']] #anomaly

ax.plot(df['Date'], df['BHP'], color='blue', label = 'Normal', linestyle='dashed',linewidth=2)
ax.scatter(a['Date'],a['BHP'], color='red', label = 'Anomaly',s=20,)
plt.legend()
plt.show();

"""**PREDICTION**"""

#Importing necessary libraries
import math
import pandas_datareader as web
import numpy as np
import pandas as pd 
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from sklearn.metrics import mean_squared_error
from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
df = pd.read_csv("/content/DD6P.csv")

#df.head(5)

#df.tail(5)

#df.info

#df['Date'].describe()


#data = df[['Oil rate','THP','BHP','ESP']]
df = df[df['BHP']>0]
df[df.columns[5]] = df[df.columns[5]].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int).dropna()
df

data = df.filter(['BHP'])
data = data[data['BHP']>0]
dataset = data.values
training_data_len = math.ceil(len(dataset)*.7)
training_data_len

name ='BHP'
#Plotting the the graph
plt.figure(figsize=(15,5))
plt.title('History '+ name)
plt.scatter(df['Date'],df['BHP'],c='Red')
plt.xlabel('Date', fontsize=20)
plt.ylabel('BHP (psi))', fontsize=5)
plt.show()

#sale the data
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(dataset)

scaled_data

# split into train and test sets
train_size = int(len(scaled_data) * 0.8)
test_size = len(scaled_data) - train_size
train, test = scaled_data[0:train_size, :], scaled_data[train_size:len(scaled_data), :]

# convert an array of values into a data_set matrix def
def create_data_set(_data_set, _look_back=1):
    data_x, data_y = [], []
    for i in range(len(_data_set) - _look_back - 1):
        a = _data_set[i:(i + _look_back), 0]
        data_x.append(a)
        data_y.append(_data_set[i + _look_back, 0])
    return np.array(data_x), np.array(data_y)

# reshape into X=t and Y=t+1
look_back =60
X_train,Y_train,X_test,Ytest = [],[],[],[]
X_train,Y_train=create_data_set(train,look_back)
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test,Y_test=create_data_set(test,look_back)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

# create and fit the LSTM network regressor = Sequential() 
regressor = Sequential()

regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))
regressor.add(Dropout(0.1))

regressor.add(LSTM(units = 50, return_sequences = True))
regressor.add(Dropout(0.1))

regressor.add(LSTM(units = 50))
regressor.add(Dropout(0.1))

regressor.add(Dense(units = 1))


regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')
reduce_lr = ReduceLROnPlateau(monitor='val_loss',patience=5)
history =regressor.fit(X_train, Y_train, epochs = 100, batch_size = 32,validation_data=(X_test, Y_test), callbacks=[reduce_lr],shuffle=False)

train_predict = regressor.predict(X_train)
test_predict = regressor.predict(X_test)

# invert predictions
train_predict = scaler.inverse_transform(train_predict)
Y_train = scaler.inverse_transform([Y_train])
test_predict = scaler.inverse_transform(test_predict)
Y_test = scaler.inverse_transform([Y_test])

print('Train Mean Absolute Error:', mean_absolute_error(Y_train[0], train_predict[:,0]))
print('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0])))
print('Test Mean Absolute Error:', mean_absolute_error(Y_test[0], test_predict[:,0]))
print('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0])))
plt.figure(figsize=(8,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc='upper right')
plt.show();

#creatinf testing dataset
test_data = scaled_data[training_data_len - 30: , :]

#creating x_test and y_tets datasets
x_test = []
y_test = dataset[training_data_len:, :]
for i in range (30, len(test_data)):
  x_test.append(test_data[i -30:i, 0])

#converting data to numpy array
x_test = np.array(x_test)

#reshape data
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

#get predicted price values
predictions = regressor.predict(x_test)
predictions = scaler.inverse_transform(predictions)

#plot the data

train = data[:training_data_len]
valid = data[training_data_len:]
valid['Predictions'] = predictions

#visualization
plt.figure(figsize=(20,10))
plt.title('Plit random Train-Test data')
plt.xlabel('Date', fontsize=20)
plt.ylabel('Close Price USD ($)', fontsize=20)
plt.plot(train['BHP'])
plt.plot(valid[['BHP', 'Predictions']])
plt.legend(['Train', 'Val','Predictions'], loc='upper right')
plt.show()

dataset_test = data[-60:].values

inputs = dataset_test
inputs = inputs.reshape(-1,1)
inputs = scaler.transform(inputs)

i = 0
while i < 60:
    X_test = []
    no_of_sample = len(inputs)

    # Lay du lieu cuoi cung
    X_test.append(inputs[no_of_sample-60:no_of_sample, 0])
    X_test = np.array(X_test)
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

    # Du doan gia
    predicted_stock_price = regressor.predict(X_test)

    # chuyen gia tu khoang (0,1) thanh gia that
    predicted_stock_price = scaler.inverse_transform(predicted_stock_price)

    dataset_test = np.append(dataset_test, predicted_stock_price[0])
    inputs = dataset_test
    inputs = inputs.reshape(-1, 1)
    inputs = scaler.transform(inputs)
    print('Price next (' + str(i+1) + ') days : ', predicted_stock_price[0][0])
    i = i + 1

dataset_pre = np.append(data['BHP'], dataset_test[60:])

#visualization
plt.figure(figsize=(20,10))
plt.title('Split 0.7 Train - 0.3 Test data')
plt.xlabel('Date', fontsize=20)
plt.ylabel('BHPD (psi)', fontsize=20)
plt.plot(dataset_pre)
plt.plot(dataset[:])
plt.legend(['Prediction', 'History'], loc='upper right')
plt.show()

"""**Multivariate LSTM**"""

#Importing necessary libraries
import math
import pandas_datareader as web
import numpy as np
import pandas as pd 
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from sklearn.metrics import mean_squared_error
from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
df = pd.read_csv("/content/DD6P.csv")

df = df[df['BHP']>0]
df = df.set_index('Date')
#df = df.dropna(axis=1,  )
df[df.columns[5]] = df[df.columns[5]].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int).dropna()
df.dtypes

# Multivariate LSTM
# Defining the inputs and output of the LSTM model so as to create the sequences
input_1 =df['Oil rate'].values
input_2 = df['THP'].values
input_3 = df['ESP'].values
input_4 = df['GL'].values
output_feat = df['BHP'].values


# Reshaping for converting the inputs/output to 2d shape
input_1 = input_1.reshape((len(input_1), 1))
input_2 = input_2.reshape((len(input_2), 1))
input_3 = input_3.reshape((len(input_3), 1))
input_4 = input_4.reshape((len(input_4), 1))
output_feat = output_feat.reshape((len(output_feat), 1))

# Use of hstack to put together the input sequence arrays horizontally (column wise)
from numpy import hstack
df = hstack((input_1, input_2,input_3,input_4))
df[:5]

# Selecting the length of each sequence and the size of the prediction horizon (forecast_steps)
seq_len= 15
pred_horizon=60
# Splitting the dataset into training and test set (y_test -->to compare the LSTM forecasts for given inputs (X_test))
X_train=df[:-pred_horizon]
y_train=output_feat[:-pred_horizon]
X_test=df[-pred_horizon:]
y_test=output_feat[-pred_horizon:]

# The shape of training and test data
print(X_train.shape,y_train.shape)
print(X_test.shape,y_test.shape)

# MinMaxScaler is used to transform dataset columns by scaling them between 0 & 1.Training samples are first fitted 
# and then transformed, whereas the test samples are transformed based on the previously fitted training samples in order
# to avoid forecasting with a biased ML model.
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
y_train=scaler.fit_transform(y_train)
y_test=scaler.transform(y_test)

# Function to create the input-output sequence. Each train batch consists of 12 inputs & the corresponding 
# y_target value (one step-ahead)
from numpy import array

def set_seq(seq, seq_len):
    X = []
    y = []
    for t in range(len(seq)-seq_len):
        end = t + seq_len # End index is equal to the current index plus the specified number of sequence length
        
        if end> len(seq)-1:# if the length of the formed train sequence is greater than the length of the input feature,stop
            break
# for seq_length=12 : X_input seq. ->12 (indices 0-11) past observations, y_target -> 1 observation at one time step ahead
# (index 12)
        Xseq= seq[t:end, :-1]
        y_target =seq[end, -1]
        X.append(Xseq)
        y.append(y_target)
    return array(X), array(y) #initializing the arrays

# Use of hstack to put together the train sequence arrays horizontally 
df_train = hstack((X_train,y_train))
# Creating the training sequences
Xtrain_seq,ytrain_seq=set_seq(df_train, seq_len)

# The input training data have been converted into 3d shape--> [sample_length,seq_len, number of input features]
print(Xtrain_seq.shape)

# Presenting the first two training sequences. As it can be observed, the first 12 input entries (seq_len=12),
# i.e. The current index input value at time step 12 and the past 11 observations for each feature, together with the
# GOOGL Close price at time step 13 (one-step ahead),comprise the first sequence.
# In the second batch, the sequence is updated by dropping the first input values and appending the next X-y values
# at the end of the batch.
# As it can be observed, the first two y target values correspond to the y_train values with indices 12 and 13 for
# time steps 13 and 14 respectively
for t in range(2):
    print(Xtrain_seq[t], ytrain_seq[t])
print('\r')
print('The first two ytrain_seq values correspond to the  train target values (y_train) with indexes 12 and 13 : ')
print(y_train[12:14])

# Defining the number of input features
features_num = Xtrain_seq.shape[2]
features_num

# Reshaping the target train data to be inserted into the LSTM model in the required dimension
ytrain_seq=ytrain_seq.reshape((-1,1))
ytrain_seq.shape

# Importing the necessary libraries to create/construct the neural network model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,LSTM
from tensorflow.keras import initializers
import tensorflow as tf
tf.random.set_seed(0)
np.random.seed(0)
# Use of the he_uniform initializer to set the initial weights 
initializer = tf.keras.initializers.he_uniform(seed=0)
model = Sequential()
# Use of 12 neurons--> equal to the length of an input train sequence
model.add(LSTM(12, activation='relu',  input_shape=(seq_len, features_num),kernel_initializer=initializer ))

# The output layer consists of 1 neuron with a 'linear' activation fuction
model.add(Dense(1,activation='linear'))
# The model is compiled with selected loss function= 'mse', whereas the selected optimizer is 'adam' with a learning rate
# of 0.001, epsilon=1e-8 and with the default values of the exponential decay rates for the first and second moment estimates
opt = tf.keras.optimizers.Adam(learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-8)

model.compile(optimizer=opt, loss='mse')

# Fitting the LSTM model 
model.fit( Xtrain_seq, ytrain_seq,epochs=30, batch_size=1, shuffle=False,verbose=0)

# Training loss plot
loss = pd.DataFrame(model.history.history)
loss.plot()
plt.title('LSTM Training Loss',fontweight='bold')
plt.xlabel('Epochs',fontweight='bold')
plt.ylabel("Loss-'MSE'",fontweight='bold');

# Determining all LSTM training set predictions so as to compare them with the actual GOOGL Close training values 

train_lstm_outputs = []
train_batch = Xtrain_seq[0].reshape((1, seq_len, features_num))

for i in range(len(Xtrain_seq[1:])):
    train_lstm_out = model.predict(train_batch)[0]
    train_lstm_outputs.append(train_lstm_out)
    train_batch=Xtrain_seq[1:][i].reshape((1, seq_len, features_num))
    
#Append train_lstm_output from last train batch    
train_lstm_outputs.append(model.predict(train_batch)[0])

# Last appended input to the final train sequence (train_batch) is X_train[-2]. 
# The X_train[-1] is to be appended to the input sequence after training to determine the first forecasted value 
# This is because the model is trained to predict one step ahead 
print('Final train batch (sequence): \n')
print(train_batch)
print("\r")
print('Last appended input of the final train batch: \n')
print(X_train[-2])

# Applying the inverse_transform function to get the original values
predictions=scaler.inverse_transform(train_lstm_outputs)
predictions=predictions.reshape((-1,1))

# Applying the inverse_transform function to the ytrain_seq set
ytrain_seq=scaler.inverse_transform(ytrain_seq)

# LSTM Training Performance - Actual vs. Predicted Training Set Values for 198 training steps (198 training sequences)

plt.figure(figsize=(10,6))
plt.plot(ytrain_seq,marker='o',linestyle='-')

plt.plot(predictions,marker='o',linestyle='dashed')
plt.title(' LSTM (DD-6P BHP) - Actual vs. Predicted Values (Training Set)',fontweight='bold')
plt.legend(('Actual_Train_Values','Predicted_Train_Values'))
plt.xlabel('Days',fontweight='bold')
plt.ylabel('DD-6P BHP Prediction',fontweight='bold');

# Training relative Error Percentage distribution plot
trainset_error=abs((ytrain_seq-predictions)/ytrain_seq)*100
trainset_error=pd.DataFrame(trainset_error,columns=['Training Set Error'])
plt.figure(figsize=(10,6))
sns.kdeplot(trainset_error['Training Set Error'],shade=True,color='r',kernel='gau',)
plt.xlabel('Percentage of Training Set Relative Error',fontweight='bold')
plt.title('Kernel Density Estimation ',fontweight='bold');

#Summary statistics of training relative error

trainset_error.describe().transpose()

# Determining the Root Mean Squared Error of the training set predicted values and the actual_train values
RMSE=np.sqrt(mean_squared_error(ytrain_seq,predictions))
RMSE=RMSE.round(2)
RMSE

"""**Time Series Forecasting & comparison with Test**"""

# Creating the first batch to forecast the first GOOGL Close price.
# First batch consists of the final train batch, where the last X train input (X_train[-1]) is appended 
first_batch=np.append(train_batch[:,1:,:],[[X_train[-1].reshape((1,features_num))]])
first_batch=first_batch.reshape((1, seq_len, features_num))
print('First Batch - step-ahead prediction: \n ',first_batch)
print('\r')
print ('Appended Input: \n',X_train[-1])

# Determining all LSTM predicted values so as to compare them with the actual test values 
lstm_outputs = []
batch =first_batch

# loop to determine the other predictions based on the X_test inputs that are appended to the batch
for i in range(len(X_test)):
    lstm_out = model.predict(batch)[0]
    lstm_outputs.append(lstm_out) 
    
# The first row of the current batch sequence is dropped, and the next X_test input is placed at the end of the batch
    batch = np.append(batch[:,1:,:],[[X_test[i]]],axis=1)

# Applying the inverse_transform function to the predicted values to get their true values
lstm_predictions=scaler.inverse_transform(lstm_outputs)
lstm_predictions

# Applying the inverse_transform function to the y_test set
y_test=scaler.inverse_transform(y_test)

# Plot of the Test vs. Predicted results for a prediction horizon of 52 weeks
plt.figure(figsize=(10,6))
plt.plot(y_test,marker='o',linestyle='-')
plt.plot(lstm_predictions,marker='o',linestyle='dashed')
plt.title('LSTM (DD-6P BHP)- Actual vs. Forecasted Values',fontweight='bold')
plt.legend(('BHP_Test_Values','BHP_Forecast_Values'))
plt.xlabel('Test Steps',fontweight='bold')
plt.ylabel('BHP DD-6P PREDITION',fontweight='bold');

pr = pd.read_csv("/content/DD6P_Predict.csv")

pr = pr[pr['BHP']>0]
pr = pr.set_index('Date')
#df = df.dropna(axis=1,  )
pr[pr.columns[5]] = pr[pr.columns[5]].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int).dropna()
pr.dtypes

# Multivariate LSTM
# Defining the inputs and output of the LSTM model so as to create the sequences
input_1 =pr['Oil rate'].values
input_2 = pr['THP'].values
input_3 = pr['ESP'].values
input_4 = pr['GL'].values
output_feat = pr['BHP'].values


# Reshaping for converting the inputs/output to 2d shape
input_1 = input_1.reshape((len(input_1), 1))
input_2 = input_2.reshape((len(input_2), 1))
input_3 = input_3.reshape((len(input_3), 1))
input_4 = input_4.reshape((len(input_4), 1))
output_feat = output_feat.reshape((len(output_feat), 1))

# Use of hstack to put together the input sequence arrays horizontally (column wise)
from numpy import hstack
df = hstack((input_1, input_2,input_3,input_4))
df[:5]

X_pre=df[:]
y_pre=output_feat[:]

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
#X_train = scaler.fit_transform(X_train)
X_pre = scaler.fit_transform(X_pre)
#y_train=scaler.fit_transform(y_train)
y_pre=scaler.fit_transform(y_pre)

# Creating the first batch to forecast the first GOOGL Close price.
# First batch consists of the final train batch, where the last X train input (X_train[-1]) is appended 
first_batch=np.append(train_batch[:,1:,:],[[X_pre[-1].reshape((1,features_num))]])
first_batch=first_batch.reshape((1, seq_len, features_num))
print('First Batch - step-ahead prediction: \n ',first_batch)
print('\r')
print ('Appended Input: \n',X_pre[-1])

# Determining all LSTM predicted values so as to compare them with the actual test values 
lstm_outputs = []
batch = first_batch

# loop to determine the other predictions based on the X_test inputs that are appended to the batch
for i in range(len(X_pre)):
    lstm_out = model.predict(batch)[0]
    lstm_outputs.append(lstm_out) 
    
# The first row of the current batch sequence is dropped, and the next X_test input is placed at the end of the batch
    batch = np.append(batch[:,1:,:],[[df[i]]],axis=1)

# Applying the inverse_transform function to the predicted values to get their true values
lstm_predictions=scaler.inverse_transform(lstm_outputs)
lstm_predictions

# Applying the inverse_transform function to the y_test set
y_pre=scaler.inverse_transform(y_pre)

# Plot of the Test vs. Predicted results for a prediction horizon of 52 weeks
plt.figure(figsize=(10,6))
plt.plot(y_pre,marker='o',linestyle='-')
plt.plot(lstm_predictions)

plt.title('LSTM (DD-6P BHP)- Actual vs. Forecasted Values',fontweight='bold')
plt.legend(('BHP_Test_Values','BHP_Forecast_Values'))
plt.ylim(1000,2500)
plt.xlabel('Test Steps',fontweight='bold')
plt.ylabel('BHP DD-6P PREDITION',fontweight='bold');

pr['BHP_pre'] = lstm_predictions[:]
pr

# Average Weekly value Plots of Global Active Power, Global Reactive Power,Global Intensity and Voltage from 2006-12 to 2010-12
fig, axes = plt.subplots(nrows=3, ncols=2,figsize=(13,8))

pr['Oil rate'].plot(ax=axes[0,0],color='g')
axes[0,0].set_title('Oil rate',fontweight='bold',fontsize=13)


pr['THP'].plot(ax=axes[0,1],color='orange')
axes[0,1].set_title('THP',fontweight='bold',fontsize=13)


pr['GL'].plot(ax=axes[1,0],color='r')
axes[1,0].set_title('GL',fontweight='bold',fontsize=13)


pr['ESP'].plot(ax=axes[1,1],color='c')
axes[1,1].set_title('ESP',fontweight='bold',fontsize=13)

pr['BHP_pre'].plot(ax=axes[2,0],color='black')
axes[2,0].set_title('BHP_pre',fontweight='bold',fontsize=13)


for ax in axes.flat:
    
    ax.figure.tight_layout(pad=1)
    ax.set_xlabel('Date',fontweight='bold')

from mpl_toolkits.axes_grid1 import host_subplot
import mpl_toolkits.axisartist as AA
import matplotlib.pyplot as plt

host = host_subplot(111, axes_class=AA.Axes)
plt.subplots_adjust(right=0.75)

par1 = host.twinx()
par2 = host.twinx()

offset = 60
new_fixed_axis = par2.get_grid_helper().new_fixed_axis
par2.axis["right"] = new_fixed_axis(loc="right", axes=par2,
                                        offset=(offset, 0))

par2.axis["right"].toggle(all=True)

host.set_xlim(0, 2)
host.set_ylim(0, 2)

host.set_xlabel("Distance")
host.set_ylabel("Density")
par1.set_ylabel("Temperature")
par2.set_ylabel("Velocity")

p1, = host.plot([0, 1, 2], [0, 1, 2], label="Density")
p2, = par1.plot([0, 1, 2], [0, 3, 2], label="Temperature")
p3, = par2.plot([0, 1, 2], [50, 30, 15], label="Velocity")

par1.set_ylim(0, 4)
par2.set_ylim(1, 65)

host.legend()

host.axis["left"].label.set_color(p1.get_color())
par1.axis["right"].label.set_color(p2.get_color())
par2.axis["right"].label.set_color(p3.get_color())

plt.draw()
plt.show()

#plt.savefig("Test")

pr.THP.plot(label="Points", legend=True)
pr.GL.plot(secondary_y=True, label="Comments", legend=True)